\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{fontspec}
\usepackage{multirow}
%\setmainfont{Times New Roman}
\newcommand{\tabincell}[2]{\begin{tabular}{@{}#1@{}}#2\end{tabular}}
\usepackage{stfloats}
\usepackage[pagebackref=true,colorlinks,linkcolor=red,citecolor=green,breaklinks=true,bookmarks=false]{hyperref}
\cvprfinalcopy % *** Uncomment this line for the final submission
\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}
% Pages are numbered in submission mode, and unnumbered in camera-ready
%\ifcvprfinal\pagestyle{empty}\fi
\setcounter{page}{1}
\title{Flow-Guided Feature Aggregation for Video Object Detection}
\author{Hongzhi Liu\\\\
August 10, 2018}

\begin{document}
%%%%%%%%% TITLE
\maketitle
%\thispagestyle{empty}
\begin{abstract}
	Recent years have witnessed significant progress in object detection. Extending state-of-the-art object detectors from image to video is challenging. Existing work attempts to exploit temporal information on box level, but such methods	are not trained end-to-end. During preparation for URPC2018, I read a thesis written by Xizhou Zhu who is from University of Science and Technology of China. This paper presents flow-guided feature  aggregation, an accurate and end-to-end learning framework for video object detection. It leverages temporal coherence on feature level instead and improves the per-frame features by aggregation of nearby features along the motion paths, and thus improves the video recognition accuracy. This method significantly improves upon strong single frame baselines in ImageNet VID, especially for more challenging fast moving objects.
\end{abstract}
%%%%%%%%% BODY TEXT

\section{Overview of FGFA}

State-of-the-art methods of object detection share a similar two-stage structure. Deep Convolutional Neural Networks(CNNs) \cite{Krizhevsky2012ImageNet,Simonyan2014Very} are firstly applied to generate a set of feature maps over the whole input image. A shallow detection-specific network \cite{He2014Spatial,Girshick2015Fast,Ren2015Faster} then generates the detection results from the feature maps. These methods achieve excellent results in still images. However, directly applying them for video object detection is challenging.

The recognition accuracy suffers from deteriorated object appearances in videos that are seldom observed in still images, such as motion blur, video defocus, rare poses and \emph{etc.} as shown in Fig.~\ref{p1} and Fig.~\ref{p2}. Nevertheless, the video has rich information about the same object instance, which is exploited in existing video object detection methods \cite{Kang2016Object} in a simple way usually observed in multiple ``nap-shots'' in a short time.
\begin{figure}
	\begin{center}
		\includegraphics[scale=0.5]{1.png}
	\end{center}
	\caption{Illustration of FGFA. For each input frame, a feature map sensitive to ``cat'' is visualized. The feature activations are low at the reference frame t, resulting in	detection failure in the reference frame. The nearby frames $t−10$ and $t+10$ have high activations. After FGFA, the feature map at the reference frame is improved and detection on it succeeds.}
	\label{p1}
\end{figure}

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.5]{2.png}
	\end{center}
	\caption{Typical deteriorated object appearance in videos.}
	\label{p2}
\end{figure}

Xizhou Zhu and his team attempt to take a deeper look at video object detection. They seek to improve the detection or recognition quality by exploiting temporal information, in a principled way. In this paper, Zhu and his team propose flow-guided feature aggregation (FGFA). As illustrated in Fig.~\ref{p1}, the feature extraction network is applied on individual frames to produce the per-frame feature maps. To enhance the features at a reference frame, an optical flow network \cite{Dosovitskiy2015FlowNet} estimates the motions between the nearby frames and the reference frame. The feature maps from nearby frames are warped to the reference frame according to the flow motion. The warped features maps, as well as its own feature maps on the reference frame, are aggregated according to an adaptive weighting network.

Zhu's approach is evaluated on the large-scale ImageNet VID dataset \cite{Russakovsky2015ImageNet}. Rigorous ablation study verifies that it is effective and significantly improves upon strong single-frame baselines. Combination with box-level methods produces further improvement. Furthermore, they report object detection accuracy on par with the best engineered systems winning the ImageNet VID challenges, without additional bells-and-whistles.
\begin{table*}[b]
	\caption{Results of using different number of frames in training and inference, using ResNet-50. Default parameters are indicated by.}\label{t1}
	\begin{center}
		\setlength{\tabcolsep}{0.5mm}{
			\begin{tabular}{c| c c c c c c c| c c c c c c c}
				\hline
				\#training frames&\multicolumn{7}{|c|}{2}& \multicolumn{7}{|c}{5}  \\
				\hline
				\# testing frames&1&5&9&13&17&21&25&1&5&9&13&17&21&25\\
				\hline
				\hline
				mAP (\%)&70.6&72.3&72.8&73.4&73.7&74.0&74.1&70.6&72.4&72.9&73.3&73.6&74.1&74.1\\
				\hline
				runtime(ms)&203&330&406&488&571&647&726&203&330&406&488&571&647&726\\
				\hline
			\end{tabular} }
		\end{center}
	\end{table*}


\section{Flow Guided Feature Aggregation}

In this paper, Xizhou Zhu and his team focus on another aspect of associating and assembling the rich appearance information in consecutive frames to improve the feature representation, and then the video recognition accuracy.

\subsection{A Baseline and Motivation}

Given the input video frames ${I_i}$, $i = 1,\ldots,\infty$, Zhu aims to output object bounding boxes on all the frames, ${y_i}$, $i = 1,\ldots,\infty$. A baseline approach is to apply an off-the-shelf object detector to each frame individually.

Two modules are necessary for such feature propagation and enhancement: 1) motion-guided spatial warping. It estimates the motion between frames and warps the feature maps accordingly. 2) feature aggregation module. It figures out how to properly fuse the features from multiple frames. Together with the feature extraction and detection networks, these are the building blocks of Zhu's approach.

\subsection{Model Design}

{\bfseries Flow-guided warping.} As motivated by \cite{Zhu2017Deep}, given a reference frame $I_i$ and a neighbor frame $I_j$, a flow field $M_{i\to j} = F(I_i, I_j)$ is estimated by a flow network F \cite{Dosovitskiy2015FlowNet}. The feature maps on the neighbor frame are warped to the reference frame according to the flow. The warping function is defined as Eq.~\ref{q1}.
\begin{equation}
f_{j\to i }=W(f_j,M_{i\to j})=W(f_j,F(I_i,I,j)) \label{q1}
\end{equation}
where W($\cdot$) is the bilinear warping function applied on all the locations for each channel in the feature maps, and $f_{j\to i }$ denotes the feature maps warped from frame $j$ to frame $i$.

{\bfseries Feature aggregation.} After feature warping, the reference frame accumulates multiple feature maps from nearby frames. These feature maps provide diverse information of the object instances. For aggregation, the team employ different weights at different spatial locations and let all feature channels share the same spatial weight. The 2D weight maps for warped features $f_{j\to i }$ are denoted as $w_{j\to i }$. The aggregated features at the reference frame $\bar{f_i}$ is then obtained as Eq.~\ref{q2}.
\begin{equation}
\bar{f_i}=\sum_{j=i-K}^{i+K}w_{j\to i}f_{j\to i}, \label{q2}
\end{equation}
where K specifies the range of the neighbor frames for aggregation. The equation is similar to the formula of attention models, where varying weights are assigned to the features in the memory buffer. The aggregated features $\bar{f_i}$ are then fed into the detection sub-network to obtain the results as Eq.~\ref{q3}.
\begin{equation}
y_i=\mathcal{N}_{det}(\bar{f_i}). \label{q3}
\end{equation}
Compared to the baseline and previous box level methods, their method aggregates information from multiple frames before producing the final detection results.

{\bfseries Adaptive weight.} The adaptive weight indicates the importance of all buffer frames $[I_{i−K},\ldots,I_{i+K}]$ to the reference frame $I_i$ at each spatial location. Specifically, at location p, if the warped features $f_{j\to i}(p)$ is close to the features $f_i(p)$, it is assigned to a larger weight. Otherwise, a smaller weight is assigned. Here, Xizhou Zhu uses the cosine similarity metric to measure the similarity between the warped features and the features extracted from the reference frame. Moreover, the team do not directly use the convolutional features obtained from $\mathcal{N}_{feat}(I)$. Instead, they apply a tiny fully convolutional network $\epsilon(·)$ to features $f_i$ and $f_{j\to i}$, which projects the features to a new embedding for similarity measure and is dubbed as the embedding sub-network. They estimate the weight as Eq.~\ref{q4}.
\begin{equation}
w_{j\to i}(p)=\exp(\frac{f_{j\to i}^e(p)\cdot f_i^e(p)}{|f_{j\to i}^e(p)||f_i^e(p)|}). \label{q4}
\end{equation}
where $f^e = \epsilon(f)$ denotes embedding features for similarity measurement, and the weight $w_{j\to i}$ is normalized for every spatial location p over the nearby frames, $\sum_{j=i-K}^{i+K}w_{j\to i}(p)= 1$. The estimation of weight could be viewed as the process that the cosine similarity between embedding features passes through the SoftMax operation.

%------------------------------------------------------------------------
\section{Experiments}

Due to memory issues, Zhu's team use the lightweight ResNet-50 in this experiment. They tried 2 and 5 frames in each mini-batch during SGD training, and 1, 5, 9, 13, 17, 21, and 25 frames in inference. Results in Tab.~\ref{t1} show that training with 2 and 5 frames achieves very close accuracy. This verifies the effectiveness of their temporal dropout training strategy. In inference, as expected, the accuracy improves as more frames are used. The improvement saturates at 21 frames.




{\small
\bibliographystyle{ieee}
\bibliography{egbib}
}

\end{document}
